# üß† T√©cnicas de Mem√≥ria no Paper Evo-Memory

## üìä Resumo: 3 T√©cnicas Propostas + 10 T√©cnicas Comparadas

O paper **Evo-Memory** prop√µe **3 t√©cnicas novas** e compara com **10 t√©cnicas existentes** do estado da arte.

---

## üÜï T√©cnicas Propostas pelos Autores

### 1. **ExpRecent** (Experience Recent)
- **Tipo:** Baseline simples
- **Como funciona:** Usa apenas as experi√™ncias mais recentes (sem busca sem√¢ntica)
- **Performance:** Intermedi√°ria
- **Uso:** Compara√ß√£o com outras t√©cnicas

### 2. **ExpRAG** (Experience Retrieval and Aggregation) ‚≠ê
- **Tipo:** Baseline melhorada
- **Como funciona:**
  1. Busca `k` experi√™ncias similares no banco vetorial
  2. Usa in-context learning com essas experi√™ncias
  3. Adiciona nova experi√™ncia √† mem√≥ria ap√≥s cada tarefa
- **Limita√ß√£o:** N√£o tem refinamento iterativo durante a infer√™ncia
- **Performance:** Boa (0.59-0.60 em single-turn, 0.63-0.82 em multi-turn)
- **F√≥rmula:**
  ```
  R_t = Top-k similar experiences
  y_t = LLM(x_t, R_t)
  M_{t+1} = M_t ‚à™ {nova experi√™ncia}
  ```

### 3. **ReMem** (Reasoning + Memory Refinement) ‚≠ê‚≠ê‚≠ê **PRINCIPAL**
- **Tipo:** Framework avan√ßado
- **Como funciona:**
  1. Loop de decis√£o: **Think ‚Üí Act ‚Üí Refine**
  2. **Think:** Raciocina sobre como aplicar li√ß√µes
  3. **Act:** Executa a√ß√£o (gera c√≥digo, chama ferramenta)
  4. **Refine:** Analisa resultado e refina mem√≥ria em tempo real
  5. Pode fazer m√∫ltiplas itera√ß√µes de Think/Refine antes de Act
- **Diferencial:** Mem√≥ria evolui durante a resolu√ß√£o do problema (n√£o s√≥ depois)
- **Performance:** Melhor (0.58-0.65 em single-turn, **0.78-0.91 em multi-turn**)
- **F√≥rmula:**
  ```
  a^n_t ‚àà {Think, Act, Refine}
  o^n_t = Agent(x_t, M_t, a^n_t)
  ```

---

## üî¨ T√©cnicas Comparadas (Estado da Arte)

### Categoria 1: Sem Mem√≥ria Persistente

#### **ReAct** (Reasoning + Acting)
- **Tipo:** Baseline cl√°ssico
- **Como funciona:** Alterna entre raciocinar e agir
- **Limita√ß√£o:** N√£o tem mem√≥ria entre sess√µes
- **Performance:** 0.37-0.54 (baixa)

#### **Amem** (Adaptive Memory)
- **Tipo:** Mem√≥ria adaptativa b√°sica
- **Performance:** 0.56-0.63 (m√©dia)

---

### Categoria 2: Mem√≥ria Adaptativa

#### **SelfRAG** (Self-Retrieval Augmented Generation)
- **Tipo:** RAG com auto-retrieval
- **Como funciona:** Decide quando buscar informa√ß√µes
- **Performance:** 0.55-0.59 (m√©dia)

#### **MemOS** (Memory Operating System)
- **Tipo:** Sistema de mem√≥ria estruturado
- **Performance:** 0.55-0.59 (m√©dia)

#### **Mem0**
- **Tipo:** Framework de mem√≥ria para agentes
- **Performance:** 0.55-0.59 (m√©dia)

#### **LangMem**
- **Tipo:** Mem√≥ria integrada ao LangChain
- **Performance:** 0.49-0.57 (baixa-m√©dia)

---

### Categoria 3: Mem√≥ria Procedural

#### **Dynamic Cheatsheet (DC)**
- **Variantes:** DC-Cu, DC-RS
- **Tipo:** "Cola" din√¢mica de procedimentos
- **Como funciona:** Organiza conhecimento procedural em cheatsheets
- **Performance:** 0.52-0.58 (m√©dia)

#### **AWM** (Agent Workflow Memory)
- **Tipo:** Mem√≥ria baseada em workflows
- **Performance:** 0.48-0.56 (baixa-m√©dia)

---

## üìà Compara√ß√£o de Performance

### Single-Turn Tasks (Racioc√≠nio e QA)
| T√©cnica | Claude 3.7 | Gemini 2.5 | M√©dia |
|---------|-----------|------------|-------|
| **ReMem** | 0.58 | **0.65** | **0.62** |
| **ExpRAG** | **0.59** | 0.60 | **0.60** |
| ExpRecent | 0.56 | 0.58 | 0.57 |
| Amem | 0.56 | 0.63 | 0.60 |
| SelfRAG | 0.55 | 0.59 | 0.57 |
| ReAct | 0.54 | 0.37 | 0.46 |

### Multi-Turn Tasks (Ambientes Embodied)
| T√©cnica | Claude 3.7 | Gemini 2.5 | M√©dia |
|---------|-----------|------------|-------|
| **ReMem** | **0.78/0.91** | **0.50/0.64** | **0.64/0.78** |
| **ExpRAG** | 0.63/0.82 | 0.46/0.63 | 0.55/0.73 |
| ExpRecent | 0.58/0.79 | 0.39/0.59 | 0.49/0.69 |
| ReAct | 0.57/0.79 | 0.32/0.56 | 0.45/0.68 |

**Nota:** ReMem domina em tarefas multi-turn (0.78/0.91 vs 0.24/0.52 do baseline)

---